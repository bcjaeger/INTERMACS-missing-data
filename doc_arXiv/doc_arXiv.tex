\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}

\title{Improving Outcome Predictions for Patients Receiving Mechanical
Circulatory Support by Optimizing Imputation of Missing Values}

\author{
    Byron C. Jaeger
    \thanks{Source code available at
\url{https://github.com/bcjaeger/INTERMACS-missing-data}}
   \\
    Department of Biostatistics \\
    University of Alabama at Birmingham \\
  Birmingham, AL 35211 \\
  \texttt{\href{mailto:bcjaeger@uab.edu}{\nolinkurl{bcjaeger@uab.edu}}} \\
  }


\begin{document}
\maketitle

\def\tightlist{}


\begin{abstract}
\textbf{Background} Risk predictions play an important role in clinical
decision making. When developing risk prediction models, practitioners
often impute missing values to the mean. The purpose of this article is
to evaluate the impact of applying different strategies to impute
missing values on the prognostic accuracy of prediction models fitted to
the imputed data. A secondary objective was to compare the accuracy of
different imputation methods. To complete these objectives, we used data
from the Interagency Registry for Mechanically Assisted Circulatory
Support (INTERMACS). \newline\textbf{Methods and Results} We applied
thirteen different strategies to impute missing values in combination
with three different strategies to fit a risk prediction model for
mortality and transplant after receiving mechanical circulatory support.
Model performance was measured by 12-month discrimination, calibration
and net reclassification index. Results indicated that multiple
imputation consistently provided prediction models with greater
prognostic accuracy than other missing data strategies, particularly
imputation to the mean. \newline\textbf{Conclusion} Selecting an optimal
strategy to handle missing values impacts the prognostic accuracy of
downstream models. In the current analysis, multiple imputation emerged
as an optimal strategy to handle missing values in the INTERMACS data.
The current study shows that evaluation and selection of an optimal
strategy to impute missing data has the potential to improve prognostic
accuracy of risk predictions for other longitudinal registries.
\end{abstract}

\keywords{
    Missing Data,
   \and
    INTERMACS,
   \and
    imputation,
   \and
    heart failure,
   \and
    mortality,
   \and
    risk prediction
  }

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\label{sec:introduction}

\linenumbers
\doublespacing

Heart disease is a leading cause of death in the United States. Heart
failure, a primary component of heart disease, affects over 6 million
Americans, and for \textasciitilde10\% of these patients medical
management is no longer effective
\cite{benjamin2017heart,national2017health}. Mechanical circulatory
support (MCS) is a surgical intervention in which a mechanical device is
implanted in parallel to the heart to improve circulation
\cite{patel2014contemporary}. Typically, MCS is used while a patient
waits for a heart transplant (bridge-to-transplant) or in some cases as
an alternative to transplant (destination therapy)
\cite{slaughter2009advanced}. Over 250,000 patients could benefit from
MCS \cite{miller2011left}. However, less than 4,000 new patients receive
a long-term MCS device each year, with widely heterogeneous outcomes
\cite{stewart2011keeping}. The 2-year survival on MCS ranges from 61\%
for destination therapy to 78\% for bridge-to-transplant
\cite{patel2014contemporary}. Therefore, there is great need for
reliable predictions of patient-specific risk to experience adverse
events after receiving MCS. This information can be used to improve
patient selection for MCS, inform the design of next generation pumps,
and refine patient management strategies.

The Interagency Registry for Mechanically Assisted Circulatory Support
(INTERMACS) was launched to improve MCS patient outcomes through the
collection of patient characteristics, medical events, and long terms
outcomes at a nation wide level. Currently, INTERMACS comprises over
20,000 patients who have received a MCS device. As the largest registry
for data on patients receiving MCS devices, INTERMACS has been leveraged
to develop numerous risk prediction models for mortality and other types
of adverse events that may occur after receiving a device
\cite{kirklin2017eighth, kormos2019society, Adamo950}. A critical step
for developing risk prediction models is handling missing data, which is
common in a ``real world'' database that is not designed for the rigor
of data completeness found in a clinical trial. While some missing
values are related to data entry error, others may relate to instability
of a patient's circulatory status at the time of implant. For instance,
hemodynamic laboratory values are missing for patients in whom invasive
catheterizations were not performed. Patients without invasive
cathaderizations may be too ill (unstable) to tolerate the procedure.
Instead, such patients may, after determination of basic hemodynamics,
proceed directly to device placement.

The primary aim of this article is to quantify how much the prognostic
value of a risk prediction model developed from the INTERMACS registry
depends on the strategy that was applied to impute missing data prior to
developing the model. A secondary aim is to measure imputation accuracy
of each strategy by introducing varying levels of artificial missing
data and then imputing it. Because imputation to the mean has been a
standard method for multiple annual summaries of the INTERMACS data, we
measure the potential improvement in prognostic value of a risk
prediction model when other strategies are applied to impute missing
data \emph{instead of imputation to the mean}. The over-arching aim is
to clarify which imputation strategies are most likely to improve risk
prediction (and by extension, quality of care) for patients who receive
MCS devices. This investigation can directly inform future analyses of
INTERMACS data and provide evidence quantifying the benefit of imputing
missing data with sound methodology.

\hypertarget{methods}{%
\section{Methods}\label{methods}}

\label{sec:methods}

Here we describe the INTERMACS Registry (Section \ref{subsec:intermacs})
and give a broad overview of general objectives for imputation of
missing data in the context of statistical inference and supervised
statistical learning (Section \ref{subsec:inference_and_learning}). We
then describe thirteen strategies to handle missing values (Section
\ref{subsec:imputation}) and three strategies to develop risk prediction
models after missing data have been imputed (Section
\ref{subsec:modeling}). Next, we describe how risk prediction models and
by extension the imputation strategy applied before training the model
were evaluated in Section \ref{subsec:evaluation}. We outline an
internal validation procedure that was applied to compare combinations
of imputation and modeling strategies using hierarchical Bayesian models
in Section \ref{subsec:internal}. Last, details on computation and
software packages used for the current analysis are described in Section
\ref{subsec:computing}

\hypertarget{intermacs-registry}{%
\subsection{INTERMACS Registry}\label{intermacs-registry}}

\label{subsec:intermacs}

INTERMACS is a North American observational registry for patients
receiving MCS devices that began as a partnership between the National
Heart, Lung, and Blood Institute, US Food and Drug Administration, the
Centers for Medicaid and Medicare Services, industry, and individual
hospitals with the mission of improving MCS outcomes. In 2018, INTERMACS
became an official Society of Thoracic Surgeons database.

The current analysis was conducted using publicly available data
provided by the National Heart, Lung, and Blood Institute. We included a
contemporary cohort of 14,738 patients who received continuous flow LVAD
from 2012-2017. Patient follow-up begins after implantation of a
durable, long term MCS device and continues while the device is in
place. Registry endpoints include death on a device, heart
transplantation, or cessation of support (for recovery and non-recovery
reasons). INTERMACS collects pre-implant patient characteristics,
medical status, laboratory values, and many other variables. Data is
collected at regularly scheduled follow-up as well as during adverse
events such as re-hospitalization. This is a secondary analysis of
de-identified data obtained from the National Heart Lung and Blood
Institute. Primary data collection is approved through University of
Alabama Institutional Review Board and at individual sites.

\hypertarget{statistical-inference-and-learning-with-missing-data}{%
\subsection{Statistical Inference and Learning with Missing
data}\label{statistical-inference-and-learning-with-missing-data}}

\label{subsec:inference_and_learning}

\paragraph{Statistical Inference}

Previous research has thoroughly investigated statistical inference in
the context of missing data. In this setting, analysts often create
multiple imputed datasets, replicate an analysis in each of them, and
then pool their results to obtain valid test statistics for hypothesis
testing \cite{rubin2004multiple}. Imputation strategies that create a
single imputed dataset (\textit{e.g., }imputation to the mean) have been
shown to increase type I errors (\textit{i.e., }rejecting a true null
hypothesis) for inferential statistics by artificially reducing the
variance of observed data and ignoring the uncertainty attributed to
missing values \cite{van2018flexible}. To ensure valid inference,
imputation models should leverage outcome variables to predict missing
predictors \cite{sterne2009multiple}. When very few data are missing,
analysts may apply listwise deletion, \textit{i.e., }removing any
observation with at least one missing value. However, listwise deletion
can easily lead to biased inference \cite{van2020rebutting}.

\paragraph{Statistical Learning}

In the presence of missing data, the goal of supervised statistical
learning is to develop a prediction function for an outcome variable
that accurately generalizes to testing data, which may or may not
contain missing values \cite{hastie2009elements}. Because testing data
may contain missing values, listwise deletion is not a feasible strategy
for statistical learning tasks. In contrast to statistical inference,
strategies that create a single imputed dataset are often used for
statistical learning \cite{kuhn2019feature}. Previous work has
emphasized the importance of imputation strategies with greater
accuracy, suggesting that more accurate imputation strategies lead to
better performance of downstream models (\textit{i.e., }models fitted to
the imputed data) \cite{jerez2010missing}. Others have shown that
imputation to the mean, model-based imputation, and multiple imputation
can provide Bayes-optimal prediction models provided certain assumptions
are true, but these assumptions are difficult to validate in applied
settings \cite{josse2019consistency}.

Though the outcome variable may be useful for imputing predictor values,
using outcome variables for this purpose in supervised learning projects
can lead to issues in clinical implementation. For instance, suppose we
seek to predict an outcome \(Y\) for a patient in the clinical setting.
If the patient is missing information for a predictor \(X\) and our only
method to impute missing values of \(X\) leverages the observed value of
\(Y\), how should we impute \(X\)? If we do not already know \(Y\),
which would seem likely for clinical application of a prediction model,
then we cannot impute \(X\). On the other hand, if we already know
\(Y\), then we obviously do not need to predict it. Due to these
practical considerations and because INTERMACS is often leveraged to
create prediction models for clinical practice \cite{thomas2014pre}, we
did not leverage outcome variables (\textit{i.e., }time until
transplant, death, or last contact) to impute missing values of
predictors in the current analysis.

\hypertarget{missing-data-strategies}{%
\subsection{Missing Data Strategies}\label{missing-data-strategies}}

\label{subsec:imputation}

\paragraph{Single and multiple imputation}

Several of the imputation methods we considered allowed for creation of
one or many imputed datasets. For downstream models fitted to multiple
sets of imputed data, we applied the modeling technique to each imputed
training set, separately, and then used a pooling technique to generate
a single set of predictions based on multiple imputed testing data.
Specifically, we created 10 imputed training and testing sets, then
applied a modeling procedure to each imputed training set and computed
model predictions on the corresponding imputed testing set, which led to
10 sets of predictions. To aggregate these predictions, we computed the
median for each observation. Informal experiments where the mean was
used instead of the median to aggregate predictions showed little or no
difference in the model's prediction accuracy.

\paragraph{Imputation to the mean}

Imputing data to the mean involves three steps. First, numeric and
nominal variables are identified. Second, for each numeric variable, the
mean is computed and used to impute missing values in the corresponding
variable. Third, for each nominal variable, the mode is computed
(because one cannot compute a mean for categorical variables) and used
to impute missing values in the corresponding variable. The computed
means and modes are then stored for future imputation of testing data.
Because imputation to the mean is frequently used in practice, we use it
as a reference for comparison of all other strategies to impute missing
values,

\paragraph{Bayesian Regression}

Imputation with Bayesian regression draws imputed values from the
posterior distribution of model parameters, accounting for uncertainty
in both model error and estimation \cite{rubin2004multiple}. Multiple
imputation with chained equations (MICE), a well known technique for
generating multiply imputed data
\cite{azur2011multiple, van2018flexible, van2006fully}. In each
iteration of MICE, each specified variable in the dataset is imputed
using the other variables in the dataset. These iterations are run until
convergence criteria have been met.

\paragraph{Predictive Mean Matching (PMM)}

PMM computes predicted values from a pre-specified model that treats one
incomplete column in the data, \(X\), as a dependent variable. For each
missing value in \(X\), PMM identifies a set of candidate donors based
on distance in predicted values of \(X\) \cite{landerman1997empirical}.
One donor is randomly drawn from the candidates, and the observed value
of the donor is taken to replace the missing value. When missing values
were imputed using PMM, we applied MICE to form multiple imputed
datasets. For consistency with other approaches, we also generated a
single imputed dataset using PMM by randomly selecting one of the
multiple datasets imputed.

\paragraph{K-Nearest-Neighbors (KNN)}

KNN imputation identifies \(k\) `similar' observations
(\textit{i.e., }neighbors) for each observation with a missing value in
a given variable \cite{chen2000nearest}. A donor value for the current
missing value is generated by sampling or aggregating the values from
the \(k\) nearest neighbors. In the current analysis, we identified 10
nearest neighbors using Gower's distance \cite{gower}. When imputing a
single dataset using KNN, we aggregated values from nearest neighbors
using the median for numeric variables and the mode for categorical
variables. When imputing multiple datasets using KNN, we sampled one
value at random from nearest neighbors for each imputed set.

\paragraph{Hot Deck}

Similar to KNN, hot deck imputation finds \(k\) similar observations for
each observation with a missing value in a given variable
\cite{andridge2010hotdeck}. However, hot deck imputation uses a less
computationally intensive approach, either identifying neighbors at
random or using a subset of variables to find similar observations. When
imputing a single dataset using hot deck imputation, we used a selection
of 5 variables simultaneously to identify nearest neighbors. When
imputing multiple datasets using hot deck imputation, we used a separate
numeric variable for each dataset.

\paragraph{Random forests}

Random forests grow an ensemble of de-correlated decision trees, where
each tree is grown using a bootstrapped replicate of the original
training data
\cite{Breiman2001, hothorn2006survival, strobl2007bias, strobl2008conditional, ishwaran2008random, jaeger2019oblique}.
A particularly helpful feature of random forests is their ability to
estimate testing error by aggregating each decision tree's prediction
error on data outside of their bootstrapped sample
(\textit{i.e., }out-of-bag error). In the current analysis, we conduct
MICE using one random forest to impute each variable, separately. For
each imputed dataset, we allowed random forests to be re-fitted until
out-of-bag error stabilized or a maximum number of iterations was
completed. When imputing a single dataset, we used 250 trees per forest
and a maximum of 10 iterations. When imputing multiple datasets, we used
50 trees per forest and applied PMM using the random forest's predicted
values to impute missing data by sampling one value from a pool of 10
potential donors.

\paragraph{Missingness incorporated as an attribute (MIA)}

MIA is a technique that uses missing status as a predictor rather than
explicitly imputing missing values
\cite{twala2009empirical, ding2010investigation}. MIA adds another
category to nominal variables: ``Missing''. For numeric variables, MIA
creates two columns: one where missing values are imputed with positive
infinity and the other with negative infinity. Since PH models are not
compatible with infinite values, we only use MIA in boosting models.
When a decision tree uses a finite cut-point to split a given numeric
variable, it will assess the cut-point using both the positive and
negative infinite imputed columns, and utilize whichever column provides
the best split of the current data. This procedure translates to sending
all missing values to the left or to the right when forming two new
nodes of the decision tree, using whichever direction results in a
better split.

\paragraph{Assumptions}

Each missing data strategy poses different assumptions regarding the
data and the mechanisms that lead to missing values in the data.
Imputation using Bayesian regression makes the same distributional
assumptions as the Bayesian models that are applied. The
\texttt{miceRanger} algorithm does not make any formal distributional
assumptions, as random forests are non-parametric and can thus handle
skewed and multi-modal data as well as categorical data that are ordinal
or non-ordinal. Hot deck and KNN imputation also do not make
distributional assumptions, but implicitly assume that a missing value
for a given observation can be approximated by aggregating observed
values from the \(k\) most similar observations. PMM makes a similar
implicit assumption, but is slightly more robust to skewed or
multi-modal data because imputed values are sampled directly from
observed ones. MIA operates based on an implicit assumption that
missingness itself is informative. It is difficult to validate these
assumptions in applied settings and also likely that downstream models
will perform poorly if an imputation technique's assumptions are
invalid.

\hypertarget{evaluating-imputation-accuracy}{%
\subsection{Evaluating imputation
accuracy}\label{evaluating-imputation-accuracy}}

\label{subsec:imputation_accuracy}

Imputation accuracy was computed for each numerical and nominal
variable, separately. Numeric variable imputation accuracy was measured
using a re-scaled mean-squared error:
\[1 - \frac{\textrm{MSE}(\textrm{current imputation method})}{\textrm{MSE}(\textrm{imputation to the mean})}.\]
This score is greater than 0 if
\(\textrm{MSE}(\textrm{current imputation method})\) is smaller than
\(\textrm{MSE}(\textrm{imputation to the mean})\), equal to 0 if the two
MSEs are equal, and less than 0 if
\(\textrm{MSE}(\textrm{current imputation method})\) is greater than
\(\textrm{MSE}(\textrm{imputation to the mean})\). Nominal variable
imputation accuracy was measured using a re-scaled classification
accuracy:
\[1 - \frac{\textrm{Classification error}(\textrm{current imputation method})}{\textrm{Classification error}(\textrm{imputation to the mean})}.\]
This score is greater than 0 if the classification error of the current
imputation method is less than that of imputation to the mean, equal to
0 if the two classification errors are equal, and less than 0 of the
classification error of the current imputation method is worse than
imputation to the mean. These numeric and nominal scores are analogous
to the more well known \(R^2\) and Kappa statistics, respectively, but
are modified slightly in the current analysis so that imputation to the
mean will always have a score of 0. This modification makes it easier to
compare the accuracy of each imputation strategy directly with
imputation to the mean.

\hypertarget{risk-prediction-models}{%
\subsection{Risk Prediction Models}\label{risk-prediction-models}}

\label{subsec:modeling}

We applied two modeling strategies after imputing missing values:

\begin{itemize}
\tightlist
\item
  Cox proportional hazards (PH) model with forward stepwise variable
  selection
\item
  Gradient boosted decision trees (hereafter referred to as `boosting')
\end{itemize}

A thorough description of stepwise variable selection and boosting can
be found in Sections 6.1.2 and 8.2.2, respectively, of
\emph{Introduction to Statistical Learning}
\cite{james2013introduction}. Sir David Cox's PH model is one of the
most frequently applied methods for the analysis of right-censored
time-to-event outcomes \cite{kleinbaum2010survival}. According to the PH
assumption, the effect of a unit increase in a predictor is
multiplicative with respect to a baseline hazard function. Boosting
grows a sequence of decision trees, each using information from the
previous trees in an attempt to correct their errors
\cite{friedman2001greedy, chen2016xgboost}.

\hypertarget{evaluation-of-predictions}{%
\subsection{Evaluation of Predictions}\label{evaluation-of-predictions}}

\label{subsec:evaluation}

\paragraph{The Brier score}

The prognostic value of each risk prediction model was primarily
assessed using the Brier score, which depends on both the discrimination
and calibration of predicted risk values
\cite{graf1999assessment, rufibach2010use}. Let \(\tilde{Y}_i(t)\)
represent the observed status of individual \(i\) at time \(t > 0\) in a
testing set of \(M\) observations. Suppose \(\tilde{Y}_i(t)=1\) if there
is an observed event at or before \(t\) and \(\tilde{Y}_i(t)=0\)
otherwise. The Brier score is computed with
\begin{equation} \label{eqn:brier_score}
\widehat{\textrm{BS}}(t)= \frac{1}{M} \sum_{i=1}^{M} \widehat{W}_i(t) \left\{ \tilde{Y}_i(t) - \widehat{S}(t \mid \bm{x}_i) \right\}^2, \end{equation}
where, for the \(i^{th}\) observation, \(\widehat{S}(t \mid \bm{x}_i)\)
is the estimated probability of survival at time \(t\) according to a
given risk prediction model, \(\bm{x}_i\) is the set of input values for
predictor variables in the model, and \(\widehat{W}_i(t)\) is the
inverse proportional censoring weight at time \(t\)
\cite{gerds2006consistent}. Thus, the Brier score is the mean squared
difference between observed event status and expected event status
according to a RPE at time \(t\). Throughout the current analysis, we
set \(t = 6\) months after receiving MCS to focus on short term risk
prediction. Models have been developed to simultaneously predict short
term and long term mortality risk after receiving MCS, but these are
beyond the scope of the current study
\cite{blackstone1986decomposition}.

\paragraph{The scaled Brier score}

The Brier score is dependent on the rate of observed events, which can
make it a difficult metric to interpret. It is often more informative to
scale the Brier score based on the Brier score of a naive model. More
specifically, for a given risk prediction model, the scaled Brier score
is computed as
\[\textrm{Scaled } \widehat{\textrm{BS}}(t)\textrm{ of model } = 1 - \frac{\widehat{\textrm{BS}}(t)\textrm{ of model}}{\widehat{\textrm{BS}}(t)\textrm{ of naive model}}.\]
As the Brier score for risk prediction is analogous to mean-squared
error for prediction of a continuous outcome, the scaled Brier score is
analogous to the \(R^2\) statistic. Similar to the \(R^2\) statistic, a
scaled \(\widehat{\textrm{BS}}(t)\) of 1.00 and 0.00 indicate a perfect
and worthless model, respectively. In our analyses, a Kaplan-Meier
estimate based on the training data (\textit{i.e., }a risk prediction
model that did not use any predictor variables) provided the naive
prediction.

\hypertarget{internal-validation-via-monte-carlo-cross-validation-mccv}{%
\subsection{Internal Validation via Monte-Carlo Cross-Validation
(MCCV)}\label{internal-validation-via-monte-carlo-cross-validation-mccv}}

\label{subsec:internal}

To assess the prognostic value of each missing data strategy, we
internally validated a total of FILL IN modeling algorithm based on
combinations of imputation strategies and modeling strategies described
in Sections \ref{subsec:imputation} and \ref{subsec:modeling}. For
convenience, we use the term `modeling algorithm' to denote the
combination of a missing data strategy and a modeling strategy
(\textit{e.g., }imputation using mean/mode values followed by fitting
the PH model with stepwise variable selection) \cite{kuhn2013applied}.
We conducted internal validation using 200 replicates of Monte-Carlo
cross validation (MCCV), a resampling technique for internal validation
(Figure \ref{fig:monte_carlo_cv}).

\paragraph{Steps taken in each MCCV replicate}

In each replicate, 50\% of the available data were used for model
training and testing. Imputation was conducted in the training and
testing sets, separately, for each imputation strategy. Some imputation
strategies (\textit{e.g., }KNN and random forests) used only the
training data to generate imputed values, whereas others
(\textit{e.g., }Bayesian regression, PMM, and hot deck)

After imputation, each modeling algorithm (\textit{i.e., }stepwise PH
models, GBDTs, and the cIF) was applied, separately, to develop a RPE
for death or transplant that incorporated up to 50 predictor variables
from the entire collection of covariates available.

\hypertarget{statistical-analysis}{%
\subsection{Statistical analysis}\label{statistical-analysis}}

Participant characteristics were tabulated for the overall population
and stratified by event status. Continuous and categorical variables
were summarized as mean (standard deviation) and percent, respectively.
Imputation accuracy was aggregated for all numeric and nominal variables
to create two overall scores for each imputed dataset, and the
distribution of these scores was tabulated for each imputation strategy.

\hypertarget{computational-details}{%
\subsection{Computational details}\label{computational-details}}

\label{subsec:computing}

implement \texttt{miceRanger}, an R package similar to
\texttt{missForest} that implements MICE with random forests by
implementing PMM \cite{missForest}. \texttt{miceRanger} follows an
iterative procedure where one random forest is fit to each variable in a
dataset, separately, and then used to impute missing values in that
variable. The procedure may repeat until estimated generalization error
of the random forest collection stabilizes.

\hypertarget{results}{%
\section{Results}\label{results}}

\paragraph{Patient Characteristics}

During the first 6 months after receiving MCS, the number (\% of overall
population) of deaths, transplants, and loss-to-follow-up events was
1,897 (13\%), 1,067 (7.2\%), and 981 (6.7\%), respectively. The mean
(standard deviation) age of patients was 57 (13) years, 66\% of patients
identified as white and 79\% were male (Table
\ref{tbl_characteristics}).

\paragraph{Imputation accuracy}

\clearpage
\begin{table}
\caption{Participant characteristics stratified by event status.}
\label{tbl_characteristics}


\begin{tabular}{lccccc}
\toprule
Characteristic & Overall & Censored & Cessation of support & Dead & Transplant\\
\midrule
No. of patients & 14,738 & 6,308 & 310 & 4,204 & 3,916\\
Follow up time, months & 18 (16) & 25 (18) & 16 (11) & 13 (14) & 13 (10)\\
Age, years & 57 (13) & 57 (13) & 48 (14) & 61 (12) & 53 (12)\\
Sex &  &  &  &  & \\
\hspace{1em}Female & 21 & 22 & 41 & 22 & 19\\
\addlinespace
\hspace{1em}Male & 79 & 78 & 59 & 78 & 81\\
Race &  &  &  &  & \\
\hspace{1em}White & 66 & 63 & 64 & 71 & 67\\
\hspace{1em}Black & 25 & 28 & 27 & 21 & 23\\
\hspace{1em}Other & 9.2 & 9.6 & 9.4 & 7.4 & 10\\
\addlinespace
Body mass index & 29 (7) & 29 (7) & 29 (8) & 29 (7) & 28 (6)\\
Device strategy &  &  &  &  & \\
\hspace{1em}Other & 0.6 & 0.6 & 2.6 & 0.5 & 0.6\\
\hspace{1em}Bridge to transplant & 52 & 43 & 51 & 40 & 81\\
\hspace{1em}Destination therapy & 47 & 56 & 46 & 60 & 18\\
\addlinespace
Device type &  &  &  &  & \\
\hspace{1em}Bi-ventricular assistance device & 3.5 & 2.1 & 0.3 & 5.8 & 3.4\\
\hspace{1em}Left-ventricular assistance device & 97 & 98 & 100 & 94 & 97\\
\bottomrule
\multicolumn{6}{l}{\rule{0pt}{1em}\textit{Note: }}\\
\multicolumn{6}{l}{\rule{0pt}{1em}Table values are mean (standard deviation) or \%}\\
\end{tabular}

\end{table}

\clearpage

\begin{table} 
 \caption{Accuracy of strategies to impute artificial missing data} 
\label{tbl_impute_accuracy} 
 
\begin{tabular}{lcccc}
\toprule
& \multicolumn{2}{c}{Nominal variables} & \multicolumn{2}{c}{Numeric variables} \\ 
 \cmidrule(lr){2-3}\cmidrule(lr){4-5}
 & Single imputation & Multiple imputation & Single imputation & Multiple imputation \\ 
\midrule
\multicolumn{1}{l}{+15\% additional missing data} \\ 
\midrule
Imputation to the mean & 0 (reference) & --- & 0 (reference) & --- \\ 
Hot deck & -0.24 (-0.34, -0.16) & -0.25 (-0.37, -0.18) & -0.35 (-0.53, -0.29) & -0.42 (-0.54, -0.36) \\ 
K-nearest-neighbors & 0.02 (-0.05, 0.15) & -0.19 (-0.29, -0.08) & 0.01 (-0.01, 0.03) & -0.33 (-0.42, -0.28) \\ 
Predictive mean matching & -0.15 (-0.34, 0.00) & -0.15 (-0.35, 0.00) & -0.25 (-0.40, -0.20) & -0.26 (-0.35, -0.21) \\ 
Random forest & -0.15 (-0.26, -0.02) & -0.15 (-0.26, -0.03) & 0.06 (0.02, 0.08) & 0.05 (0.02, 0.07) \\ 
Bayesian regression & -0.14 (-0.26, 0.01) & -0.14 (-0.27, 0.01) & -0.30 (-0.41, -0.23) & -0.30 (-0.41, -0.23) \\ 
\midrule
\multicolumn{1}{l}{+30\% additional missing data} \\ 
\midrule
Imputation to the mean & 0 (reference) & --- & 0 (reference) & --- \\ 
Hot deck & -0.25 (-0.34, -0.18) & -0.26 (-0.36, -0.18) & -0.34 (-0.46, -0.30) & -0.40 (-0.53, -0.36) \\ 
K-nearest-neighbors & -0.04 (-0.09, 0.03) & -0.24 (-0.34, -0.16) & -0.02 (-0.03, -0.01) & -0.36 (-0.44, -0.32) \\ 
Predictive mean matching & -0.18 (-0.38, -0.04) & -0.18 (-0.37, -0.04) & -0.29 (-0.46, -0.24) & -0.29 (-0.40, -0.25) \\ 
Random forest & -0.18 (-0.27, -0.07) & -0.18 (-0.28, -0.07) & -0.02 (-0.05, 0.01) & -0.01 (-0.04, 0.01) \\ 
Bayesian regression & -0.17 (-0.29, -0.04) & -0.17 (-0.29, -0.05) & -0.31 (-0.43, -0.27) & -0.31 (-0.43, -0.27) \\ 
\bottomrule
\end{tabular} 
 \end{table}

\bibliographystyle{unsrt}
\bibliography{references.bib}


\end{document}
